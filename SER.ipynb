{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "166O_5dPfinB_plh6m5TFWPKTAeMRj4xF",
      "authorship_tag": "ABX9TyPek3NP8Dnkh1H4cvyE3p2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Speech_Emotion_Recognition/blob/main/SER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z193_DWlz0ZH"
      },
      "source": [
        "#1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNm1UyJzjo9T"
      },
      "source": [
        "import os\n",
        "os.chdir('C:/Users/57lzhang.US04WW4008/PycharmProjects/Speech_Emotion_Recognition')\n",
        "from data_handler import *\n",
        "from audio_processor import AudioProcessor\n",
        "import tensorflow as tf\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D, BatchNormalization, Input, Add, Activation, \\\n",
        "    MaxPooling1D, Dropout, Flatten, TimeDistributed, Bidirectional, Dense, LSTM, ZeroPadding1D, \\\n",
        "    AveragePooling1D, GlobalAveragePooling1D, Concatenate, Permute, Dot, Multiply, RepeatVector, \\\n",
        "    Lambda, Average, GlobalAveragePooling2D, DepthwiseConv2D, MaxPooling2D, ZeroPadding2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgbXiOYG8dJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb751d0-de01-47e8-92a7-8da0c100206d"
      },
      "source": [
        " ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsSU69jljuFN"
      },
      "source": [
        "# Test with speech recognition data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyDtbDlupxJV"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import soundfile as sf\n",
        "import glob\n",
        "\n",
        "\"\"\"\n",
        "Notes:\n",
        "1. In oder to use DataHandler, the data should be stored as db_name/raw_data.\n",
        "For example, RAVDESS raw data should be stored as RAVDESS/raw_data/Actor_01/*.wav;\n",
        "EMODB raw data should be stored as EMODB/raw_data/*.wav;\n",
        "2. functions with name that end with _tensor are for building tensorflow graphs purpose;\n",
        "for standard python operation, use the non-tensor counterparts \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DataHandlers:\n",
        "\n",
        "    def __init__(self, raw_data_path: str, train_ratio: float, val_ratio: float, res_freq: int,\n",
        "                 block_span: int, stride_span: int, random_seed: int, db_name: str):\n",
        "        self.raw_data_path = raw_data_path\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.res_freq = res_freq\n",
        "        self.block_span = block_span\n",
        "        self.stride_span = stride_span\n",
        "        self.random_seed = random_seed\n",
        "        self.fn_dic = {}\n",
        "        self.train_fn_dic = {}\n",
        "        self.val_fn_dic = {}\n",
        "        self.test_fn_dic = {}\n",
        "        self.db_name = db_name\n",
        "        self._create_block_fn()\n",
        "\n",
        "    def _create_block_fn(self):\n",
        "        \"\"\"\n",
        "        :return: generators and dictionaries containing train, validation and test data\n",
        "        \"\"\"\n",
        "        if self.db_name == 'RAVDESS':\n",
        "            # e.g. folder: Actor_01\n",
        "            for folder in os.listdir(self.raw_data_path):\n",
        "                # e.g. data_path: data/Actor_01\n",
        "                data_path = os.path.join(self.raw_data_path, folder)\n",
        "                fn_lst = os.listdir(data_path)\n",
        "                for idx in range(len(fn_lst)):\n",
        "                    # e.g. fn_path: data/Actor_01/03-01-01-01-01-01-02.wav\n",
        "                    fn_path = os.path.join(data_path, fn_lst[idx])\n",
        "                    label = int(fn_lst[idx][6:8]) - 1\n",
        "                    if label not in self.fn_dic:\n",
        "                        self.fn_dic[label] = [fn_path]\n",
        "                    else:\n",
        "                        self.fn_dic[label].append(fn_path)\n",
        "                # fn_dic is created to be {'00':[*.wav], '01':[*.wav], ...}\n",
        "\n",
        "                for label, fn_lst in self.fn_dic.items():\n",
        "                    np.random.seed(seed=self.random_seed)\n",
        "                    np.random.shuffle(fn_lst)\n",
        "                    self.val_fn_dic[label] = fn_lst[:int(self.train_ratio * self.val_ratio * len(fn_lst))]\n",
        "                    self.train_fn_dic[label] = fn_lst[int(self.train_ratio * self.val_ratio * len(fn_lst)):\n",
        "                                                      int(self.train_ratio * len(fn_lst))]\n",
        "                    self.test_fn_dic[label] = fn_lst[int(self.train_ratio * len(fn_lst)):]\n",
        "\n",
        "        if self.db_name == 'EMODB':\n",
        "            for fn in os.listdir(self.raw_data_path):\n",
        "                fn_path = os.path.join(self.raw_data_path, fn)\n",
        "                # W: anger; L: boredom; E: disgust; A: anxiety/fear; F: happiness; T: sadness;\n",
        "                # N: neutral\n",
        "                conversion_dict = {'W': 0, 'L': 1, 'E': 2, 'A': 3,\n",
        "                                   'F': 4, 'T': 5, 'N': 6}\n",
        "                label = conversion_dict[fn[5]]\n",
        "                if label not in self.fn_dic:\n",
        "                    self.fn_dic[label] = [fn_path]\n",
        "                else:\n",
        "                    self.fn_dic[label].append(fn_path)\n",
        "\n",
        "            for label, fn_lst in self.fn_dic.items():\n",
        "                np.random.seed(seed=self.random_seed)\n",
        "                np.random.shuffle(fn_lst)\n",
        "                self.val_fn_dic[label] = fn_lst[:int(self.train_ratio * self.val_ratio * len(fn_lst))]\n",
        "                self.train_fn_dic[label] = fn_lst[int(self.train_ratio * self.val_ratio * len(fn_lst)):\n",
        "                                                  int(self.train_ratio * len(fn_lst))]\n",
        "                self.test_fn_dic[label] = fn_lst[int(self.train_ratio * len(fn_lst)):]\n",
        "\n",
        "    def create_label_folder(self):\n",
        "        self._convert_to_block(self.train_fn_dic, 'train')\n",
        "        self._convert_to_block(self.val_fn_dic, 'val')\n",
        "        self._convert_to_block(self.test_fn_dic, 'test')\n",
        "\n",
        "        # if do_train_val_split:\n",
        "        #   val_path = os.path.join('data', 'val')\n",
        "        #   if not os.path.exists(val_path):\n",
        "        #       os.mkdir(val_path)\n",
        "        #   train_path = os.path.join('data', 'train')\n",
        "        #   for label_path in os.listdir(train_path):\n",
        "        #       # e.g. train_label_path: data/train/0/\n",
        "        #       train_label_path = os.path.join(train_path, label_path, '*.wav')\n",
        "        #       fn_lst = glob.glob(train_label_path)\n",
        "\n",
        "    #\n",
        "    #       np.random.seed(seed=self.random_seed)\n",
        "    #       np.random.shuffle(fn_lst)\n",
        "    #       val_fn_lst = fn_lst[:int(len(fn_lst) * self.val_ratio)]\n",
        "    #       target_val_fn_path = os.path.join(val_path, label_path)\n",
        "    #       if not os.path.exists(target_val_fn_path):\n",
        "    #           os.mkdir(target_val_fn_path)\n",
        "    #       for val_fn in val_fn_lst:\n",
        "    #           parts = val_fn.split(os.path.sep)\n",
        "    #           wav_fn = parts[-1]\n",
        "    #           shutil.move(val_fn, os.path.join(target_val_fn_path, wav_fn))\n",
        "\n",
        "    def _convert_to_block(self, name_fn_dic, name):\n",
        "        # e.g. data_root: RAVDESS/data\n",
        "        data_root = os.path.join(self.db_name, 'data')\n",
        "        if not os.path.exists(data_root):\n",
        "            os.mkdir(data_root)\n",
        "\n",
        "        print(f'\\nCreating {name} data...')\n",
        "        # e.g. name_path: RVDESS/data/train\n",
        "        name_path = os.path.join(data_root, name)\n",
        "        if not os.path.exists(name_path):\n",
        "            os.mkdir(name_path)\n",
        "\n",
        "        # e.g. fn_lst: [RAVDESS/raw_data/Actor_01/03-01-01-01-01-01-02.wav, data/Actor_02/*.wav, ...]\n",
        "        for label, fn_lst in name_fn_dic.items():\n",
        "            print(f'\\nProcessing {name} label {label}..')\n",
        "            # e.g. label_folder_path: RAVDESS/data/train/0\n",
        "            label_folder_path = os.path.join(name_path, str(label))\n",
        "            if not os.path.exists(label_folder_path):\n",
        "                os.mkdir(label_folder_path)\n",
        "            for i in range(len(fn_lst)):\n",
        "                if i % 10 == 0:\n",
        "                    print(f'working on {i}th file')\n",
        "                y, sr = librosa.load(fn_lst[i], sr=self.res_freq)\n",
        "                signal, _ = librosa.effects.trim(y)\n",
        "                block_len = self.res_freq * self.block_span\n",
        "                stride_len = int(self.res_freq * self.stride_span / 1000)\n",
        "                for j in range(0, len(signal), stride_len):\n",
        "                    if j + block_len > len(signal):\n",
        "                        break\n",
        "                    block_signal = signal[i:i + block_len]\n",
        "                    parts = fn_lst[i].split(os.path.sep)\n",
        "                    sf.write(label_folder_path + '/' + parts[-1][:-4] + '_' + str(j) + '.wav',\n",
        "                             block_signal, self.res_freq)\n",
        "\n",
        "    def calculate_mean_std(self):\n",
        "        \"\"\"\n",
        "        Calculate the mean and standard deviation of the train audio waveform,\n",
        "        this will be useful to normalize the input data to the model\n",
        "        \"\"\"\n",
        "        def waveform_generator(filename_dic: dict):\n",
        "            for _, fn_lst in filename_dic.items():\n",
        "                for fn in fn_lst:\n",
        "                    signal, _ = librosa.load(fn, sr=None)\n",
        "                    yield signal\n",
        "\n",
        "        signals = waveform_generator(self.train_fn_dic)\n",
        "        n = 0\n",
        "        # need to record both E(x) and E(x**2) to calculate Variance (aka. std**2)\n",
        "        Sum = square_Sum = 0\n",
        "        for signal in signals:\n",
        "            Sum += np.sum(signal)\n",
        "            square_signal = np.square(signal)\n",
        "            square_Sum += np.sum(square_signal)\n",
        "            n += len(signal)\n",
        "\n",
        "        mean = Sum / n\n",
        "        square_mean = square_Sum / n\n",
        "        # Var(X) = E(X**2) - E(X)**2\n",
        "        std = np.sqrt(square_mean - mean ** 2)\n",
        "        return mean, std\n",
        "\n",
        "    @staticmethod\n",
        "    def get_waveform_and_label(file_path):\n",
        "        parts = file_path.split(os.path.sep)\n",
        "        label = int(parts[-2])\n",
        "        waveform, _ = librosa.load(file_path, sr=None)\n",
        "        return waveform, label\n",
        "\n",
        "    def get_waveform_and_label_tensor(self, file_path):\n",
        "        parts = tf.strings.split(file_path, os.path.sep)\n",
        "        label = tf.strings.to_number(parts[-2], out_type=tf.float32)\n",
        "        #label = tf.one_hot(label, 7)\n",
        "        audio_binary = tf.io.read_file(file_path)\n",
        "        waveform, _ = tf.audio.decode_wav(audio_binary)\n",
        "        waveform = tf.reshape(waveform, [self.block_span * self.res_freq])\n",
        "        return waveform, label\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filenames(data_dir):\n",
        "        filenames = glob.glob(str(data_dir) + '/*/*')\n",
        "        np.random.shuffle(filenames)\n",
        "        num_samples = len(filenames)\n",
        "        print('Number of total examples:', num_samples)\n",
        "        print('Example file:', filenames[:20])\n",
        "        return filenames, num_samples\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filenames_tensor(data_dir):\n",
        "        filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "        filenames = tf.random.shuffle(filenames)\n",
        "        num_samples = len(filenames)\n",
        "        print('Number of total examples:', num_samples)\n",
        "        print('Example file tensor:', filenames[:20])\n",
        "        return filenames, num_samples\n",
        "\n",
        "    def count_label(self):\n",
        "        def _count_labels_from_raw_file(dic):\n",
        "            for label, fn_lst in dic.items():\n",
        "                print(f'There are {len(fn_lst)} files with label {label}')\n",
        "\n",
        "        def _count_block_labels(name: str):\n",
        "            # e.g. path: 'EMODB/data/train'\n",
        "            path = os.path.join(self.db_name, 'data', name)\n",
        "            filenames = glob.glob(path + '/*/*')\n",
        "            total_num_blocks = len(filenames)\n",
        "            print(f'There are in total {total_num_blocks} {self.block_span}s blocks')\n",
        "            for root, labels, _ in os.walk(path):\n",
        "                for label in labels:\n",
        "                    file_path = os.path.join(root, label)\n",
        "                    for _, _, files in os.walk(file_path):\n",
        "                        print(f'There are {round(len(files) / total_num_blocks, 2) * 100}% '\n",
        "                              f'files with label {label}')\n",
        "\n",
        "        print('\\nRAW wav files analysis:')\n",
        "        print('\\nIn raw dataset:')\n",
        "        _count_labels_from_raw_file(self.fn_dic)\n",
        "        print('\\nIn training raw dataset:')\n",
        "        _count_labels_from_raw_file(self.train_fn_dic)\n",
        "        print('\\nIn validation raw dataset:')\n",
        "        _count_labels_from_raw_file(self.val_fn_dic)\n",
        "        print('\\nIn testing rwa dataset:')\n",
        "        _count_labels_from_raw_file(self.test_fn_dic)\n",
        "\n",
        "        print('\\n\\n\\n Block prospect analysis:')\n",
        "        print('\\nIn training block dataset:')\n",
        "        _count_block_labels('train')\n",
        "        print('\\nIn validation block dataset:')\n",
        "        _count_block_labels('val')\n",
        "        print('\\nIn testing block dataset:')\n",
        "        _count_block_labels('test')\n",
        "    \n",
        "        return [_ for _ in self.fn_dic.keys()]\n",
        "\n",
        "    # TODO: add functions to analyze data feature distributions, and label counts\n",
        "    # Will rely on audio_processor's feature extraction functions\n",
        "    def data_feature_analysis(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g0PwbPUpp0j"
      },
      "source": [
        "class AudioProcessors:\n",
        "    def __init__(self, sample_freq, slice_span, overlap_ratio, n_mels, snr):\n",
        "        self.sample_freq = sample_freq\n",
        "        self.slice_span = slice_span\n",
        "        self.overlap_ratio = overlap_ratio\n",
        "        self.n_mels = n_mels\n",
        "        self.snr = snr\n",
        "\n",
        "    @property\n",
        "    def n_per_seg(self):\n",
        "        return int(self.slice_span / 1000 * self.sample_freq)\n",
        "\n",
        "    @property\n",
        "    def n_fft(self):\n",
        "        return int(pow(2, np.ceil(np.log(self.n_per_seg) / np.log(2))))\n",
        "\n",
        "    @property\n",
        "    def hop_length(self):\n",
        "        return int(self.n_fft * (1 - self.overlap_ratio))\n",
        "\n",
        "    def add_additive_white_gaussian_noise(self, data, label):\n",
        "        rms_signal = math.sqrt(np.mean(data ** 2))\n",
        "        std_noise = abs(rms_signal) / math.sqrt(10 ** (self.snr / 10))\n",
        "        noise = np.random.normal(0, std_noise, data.shape[0])\n",
        "        data = data + noise\n",
        "        return data, label\n",
        "\n",
        "    def get_add_additive_white_gaussian_noise_tensor(self, data, label):\n",
        "        data, label = tf.py_function(self.add_additive_white_gaussian_noise,\n",
        "                                     inp=[data, label],\n",
        "                                     Tout=[tf.float32, tf.float32])\n",
        "        data.set_shape(data.shape)\n",
        "        return data, label\n",
        "\n",
        "    def spectrogram(self, data, label):\n",
        "        spec = np.abs(librosa.stft(np.asarray(data), n_fft=self.n_fft,\n",
        "                                   hop_length=self.hop_length))\n",
        "        return spec, label\n",
        "\n",
        "    def get_spectrogram_tensor(self, data, label):\n",
        "        spectrogram, label = tf.py_function(self.spectrogram, inp=[data, label],\n",
        "                                            Tout=[tf.float32, tf.int64])\n",
        "        spectrogram = tf.expand_dims(spectrogram, -1)\n",
        "\n",
        "        spectrogram.set_shape(spectrogram.shape)\n",
        "        return spectrogram, label\n",
        "\n",
        "    def mel_spectrogram(self, data, label):\n",
        "        mel_spec = librosa.feature.melspectrogram(np.asarray(data),\n",
        "                                                  sr=self.sample_freq,\n",
        "                                                  n_fft=self.n_fft,\n",
        "                                                  hop_length=self.hop_length,\n",
        "                                                  n_mels=self.n_mels)\n",
        "        return mel_spec, label\n",
        "\n",
        "    def get_mel_tensor(self, data, label):\n",
        "        mel_spec, label = tf.py_function(self.mel_spectrogram, inp=[data, label],\n",
        "                                         Tout=[tf.float32, tf.int64])\n",
        "        mel_spec = tf.expand_dims(mel_spec, -1)\n",
        "\n",
        "        mel_spec.set_shape(mel_spec.shape)\n",
        "        return mel_spec, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFXe7h63qB1b"
      },
      "source": [
        "# import data generator\n",
        "raw_data_path = 'EMODB/raw_data'\n",
        "train_ratio = 0.9\n",
        "val_ratio = 0.1\n",
        "block_span = 1 # second\n",
        "stride_span = 10 # millisecond\n",
        "res_freq = 16000\n",
        "random_seed = 10\n",
        "db_name = 'EMODB'\n",
        "\n",
        "data_handler = DataHandlers(raw_data_path, train_ratio, val_ratio, \n",
        "                        res_freq, block_span, stride_span, random_seed,\n",
        "                        db_name)\n",
        "\n",
        "sample_freq = res_freq\n",
        "slice_span = 16 # millisecond\n",
        "overlap_ratio = 3/4\n",
        "n_mels = 64\n",
        "snr = 20\n",
        "audio_processor = AudioProcessors(sample_freq, slice_span, overlap_ratio, n_mels, snr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8PKWGGAjcs9"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path('data/mini_speech_commands')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl2NIunWj84g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4df7cd7-7174-4710-8bfc-26ab07037b32"
      },
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commands: ['down' 'go' 'left' 'no' 'right' 'stop' 'up' 'yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD8A_x38kBG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bead9ebe-b0ed-4782-9abf-b7e185466a6a"
      },
      "source": [
        "filenames, num_samples = data_handler.get_filenames_tensor(data_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of total examples: 8000\n",
            "Example file tensor: tf.Tensor(\n",
            "[b'data\\\\mini_speech_commands\\\\left\\\\338dacf5_nohash_1.wav'\n",
            " b'data\\\\mini_speech_commands\\\\up\\\\cb2929ce_nohash_1.wav'\n",
            " b'data\\\\mini_speech_commands\\\\stop\\\\24ed94ab_nohash_2.wav'\n",
            " b'data\\\\mini_speech_commands\\\\no\\\\5628d7b7_nohash_1.wav'\n",
            " b'data\\\\mini_speech_commands\\\\no\\\\c84f9b5c_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\down\\\\742d6431_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\go\\\\ab76ac76_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\left\\\\c22d3f18_nohash_3.wav'\n",
            " b'data\\\\mini_speech_commands\\\\no\\\\ac7840d8_nohash_1.wav'\n",
            " b'data\\\\mini_speech_commands\\\\left\\\\c120e80e_nohash_2.wav'\n",
            " b'data\\\\mini_speech_commands\\\\yes\\\\9a4d12fd_nohash_1.wav'\n",
            " b'data\\\\mini_speech_commands\\\\yes\\\\93ec8b84_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\up\\\\8ec6dab6_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\stop\\\\c0fb6812_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\go\\\\28ce0c58_nohash_2.wav'\n",
            " b'data\\\\mini_speech_commands\\\\yes\\\\4f2ab70c_nohash_4.wav'\n",
            " b'data\\\\mini_speech_commands\\\\left\\\\e1469561_nohash_2.wav'\n",
            " b'data\\\\mini_speech_commands\\\\go\\\\2da58b32_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\right\\\\ac899eb7_nohash_0.wav'\n",
            " b'data\\\\mini_speech_commands\\\\no\\\\fb9d6d23_nohash_0.wav'], shape=(20,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9p6C6xtkUsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9734c37c-e472-4174-f2d5-072f7d2312e5"
      },
      "source": [
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 800]\n",
        "test_files = filenames[-800:]\n",
        "\n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size 6400\n",
            "Validation set size 800\n",
            "Test set size 800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6NCSkz8OBxw"
      },
      "source": [
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "\n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2]\n",
        "\n",
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "  # Concatenate audio with padding so that all audio clips will be of the \n",
        "  # same length\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  return equal_length, label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWQwSMSJ2xx1"
      },
      "source": [
        "def preprocess_dataset(files):\n",
        "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "    output_ds = files_ds.map(get_waveform_and_label,\n",
        "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    output_ds = output_ds.map(audio_processor.get_mel_tensor,\n",
        "                                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    return output_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqCaS0E120Eg"
      },
      "source": [
        "batch_size = 64\n",
        "train_ds = preprocess_dataset(train_files)\n",
        "spectrogram_ds = train_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "train_ds = train_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rubbu50K7Lm"
      },
      "source": [
        "# calculate the input shape, which is needed to initialize keras model\n",
        "sample_data = np.random.rand((block_span * res_freq))\n",
        "sample_mel, _ = audio_processor.mel_spectrogram(sample_data, 1)\n",
        "sample_mel = np.expand_dims(sample_mel, -1)\n",
        "input_shape = sample_mel.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8rMGY_cLAh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21741dd9-c8d5-4b07-ae84-45e571f3ea82"
      },
      "source": [
        "input_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 251, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USlYOH-NKT7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824429f0-4111-4fa7-e83f-40a844e60311"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = preprocessing.Normalization()\n",
        "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    preprocessing.Resizing(32, 32),\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "history = model.fit(\n",
        "    train_ds, \n",
        "    validation_data=val_ds,  \n",
        "    epochs=EPOCHS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resizing_1 (Resizing)        (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "normalization_1 (Normalizati (None, 32, 32, 1)         3         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1605760   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 1,625,611\n",
            "Trainable params: 1,625,608\n",
            "Non-trainable params: 3\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 2.0673 - sparse_categorical_accuracy: 0.2012 - val_loss: 1.8920 - val_sparse_categorical_accuracy: 0.3338\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 1.7659 - sparse_categorical_accuracy: 0.3531 - val_loss: 1.5566 - val_sparse_categorical_accuracy: 0.4988\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 1.4355 - sparse_categorical_accuracy: 0.4847 - val_loss: 1.4311 - val_sparse_categorical_accuracy: 0.5512\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 1.2509 - sparse_categorical_accuracy: 0.5567 - val_loss: 1.3805 - val_sparse_categorical_accuracy: 0.5763\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 1.0876 - sparse_categorical_accuracy: 0.6164 - val_loss: 1.3175 - val_sparse_categorical_accuracy: 0.6100\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.9721 - sparse_categorical_accuracy: 0.6540 - val_loss: 1.2913 - val_sparse_categorical_accuracy: 0.6525\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.8818 - sparse_categorical_accuracy: 0.6851 - val_loss: 1.3130 - val_sparse_categorical_accuracy: 0.6625\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.8254 - sparse_categorical_accuracy: 0.7082 - val_loss: 1.3692 - val_sparse_categorical_accuracy: 0.6612\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.7408 - sparse_categorical_accuracy: 0.7378 - val_loss: 1.2591 - val_sparse_categorical_accuracy: 0.6950\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.7011 - sparse_categorical_accuracy: 0.7568 - val_loss: 1.2280 - val_sparse_categorical_accuracy: 0.6975\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.6709 - sparse_categorical_accuracy: 0.7716 - val_loss: 1.3345 - val_sparse_categorical_accuracy: 0.7013\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.6138 - sparse_categorical_accuracy: 0.7813 - val_loss: 1.3040 - val_sparse_categorical_accuracy: 0.7188\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.5837 - sparse_categorical_accuracy: 0.7919 - val_loss: 1.2479 - val_sparse_categorical_accuracy: 0.7188\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.5458 - sparse_categorical_accuracy: 0.8096 - val_loss: 1.3658 - val_sparse_categorical_accuracy: 0.6925\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.5397 - sparse_categorical_accuracy: 0.8086 - val_loss: 1.3992 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.5011 - sparse_categorical_accuracy: 0.8265 - val_loss: 1.3569 - val_sparse_categorical_accuracy: 0.7400\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.4815 - sparse_categorical_accuracy: 0.8292 - val_loss: 1.2649 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.4551 - sparse_categorical_accuracy: 0.8411 - val_loss: 1.3397 - val_sparse_categorical_accuracy: 0.7200\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.4372 - sparse_categorical_accuracy: 0.8467 - val_loss: 1.3562 - val_sparse_categorical_accuracy: 0.7337\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.4437 - sparse_categorical_accuracy: 0.8479 - val_loss: 1.3389 - val_sparse_categorical_accuracy: 0.7375\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.4277 - sparse_categorical_accuracy: 0.8551 - val_loss: 1.2045 - val_sparse_categorical_accuracy: 0.7487\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.4233 - sparse_categorical_accuracy: 0.8538 - val_loss: 1.3967 - val_sparse_categorical_accuracy: 0.7487\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8689 - val_loss: 1.4417 - val_sparse_categorical_accuracy: 0.7462\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.3960 - sparse_categorical_accuracy: 0.8635 - val_loss: 1.4671 - val_sparse_categorical_accuracy: 0.7462\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3661 - sparse_categorical_accuracy: 0.8701 - val_loss: 1.3507 - val_sparse_categorical_accuracy: 0.7475\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.3595 - sparse_categorical_accuracy: 0.8791 - val_loss: 1.6120 - val_sparse_categorical_accuracy: 0.7375\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.3358 - sparse_categorical_accuracy: 0.8818 - val_loss: 1.5225 - val_sparse_categorical_accuracy: 0.7462\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.3264 - sparse_categorical_accuracy: 0.8845 - val_loss: 1.6176 - val_sparse_categorical_accuracy: 0.7412\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.3234 - sparse_categorical_accuracy: 0.8921 - val_loss: 1.5885 - val_sparse_categorical_accuracy: 0.7350\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3109 - sparse_categorical_accuracy: 0.8919 - val_loss: 1.7954 - val_sparse_categorical_accuracy: 0.7337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgqBuJr1kV73"
      },
      "source": [
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2]\n",
        "\n",
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label\n",
        "  \n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJddCmuGnY5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422584d2-bfb0-4e62-cd3f-ff55740405f2"
      },
      "source": [
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram, label = audio_processor.spectrogram(waveform, label)\n",
        "\n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: go\n",
            "Waveform shape: (16000,)\n",
            "Spectrogram shape: (129, 251)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltqn-vCK2jAl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX4QAFMoz5MZ"
      },
      "source": [
        "#2. Data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nWTDjNSnX27"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhvYL6kej6tr"
      },
      "source": [
        "# import data generator\n",
        "raw_data_path = 'EMODB/raw_data'\n",
        "train_ratio = 0.9\n",
        "val_ratio = 0.1\n",
        "block_span = 1 # second\n",
        "stride_span = 10 # millisecond\n",
        "res_freq = 16000\n",
        "random_seed = 10\n",
        "db_name = 'EMODB'\n",
        "\n",
        "data_handler = DataHandler(raw_data_path, train_ratio, val_ratio, \n",
        "                        res_freq, block_span, stride_span, random_seed,\n",
        "                        db_name)\n",
        "\n",
        "sample_freq = res_freq\n",
        "slice_span = 16 # millisecond\n",
        "overlap_ratio = 3/4\n",
        "n_mels = 64\n",
        "snr = 20\n",
        "audio_processor = AudioProcessor(sample_freq, slice_span, overlap_ratio, n_mels, snr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONoUOV4-0jSi"
      },
      "source": [
        "Check spectrogram and mel-spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mGATIhWc0-Jm",
        "outputId": "f1cda704-a28e-4f95-9ec6-da7c09551500"
      },
      "source": [
        "waveform, label = data_handler.get_waveform_and_label(r'C:\\Users\\57lzhang.US04WW4008\\PycharmProjects\\Speech_Emotion_Recognition\\EMODB\\data\\train\\1\\03a07La_160.wav')\n",
        "plt.plot(waveform)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeAZh8fu0hz0"
      },
      "source": [
        "spectrogram, label = audio_processor.spectrogram(waveform, label)\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler.fit(spectrogram)\n",
        "print(scaler.mean_)\n",
        "normalized_spectrogram = ss.transform(spectrogram)\n",
        "librosa.display.specshow(normalized_spectrogram, sr=16000, hop_length=audio_processor.hop_length, x_axis='time', y_axis='linear');\n",
        "plt.colorbar();\n",
        "plt.title('Spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvOtkrBB9Koa",
        "outputId": "0ae98b9d-f306-4266-aecf-c3eb8c28b530"
      },
      "source": [
        "spectrogram.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(129, 251)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQyU-EO0Car6",
        "outputId": "77fc784f-5e84-4579-8105-d6942419062d"
      },
      "source": [
        "scaler.mean_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(251,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL2JChsJ361N"
      },
      "source": [
        "mel, label = audio_processor.mel_spectrogram(waveform, label)\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler.fit(mel)\n",
        "print(scaler.mean_)\n",
        "normalized_mel = ss.transform(mel)\n",
        "librosa.display.specshow(normalized_mel, sr=16000, hop_length=audio_processor.hop_length, x_axis='time', y_axis='linear');\n",
        "plt.colorbar();\n",
        "plt.title('Mel')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liUcAM7hw1fO"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbWS6PrLe1MQ",
        "outputId": "ca012db6-2da5-4195-8584-c4e700bd130d"
      },
      "source": [
        "sample_data = np.random.rand((block_span*res_freq))\n",
        "sample_mel, _ = audio_processor.mel_spectrogram(sample_data, 1)\n",
        "sample_mel = np.expand_dims(sample_mel, -1)\n",
        "input_shape = sample_mel.shape\n",
        "print(input_shape)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "train_filenames, train_num_samples = data_handler.get_filenames('data/train')\n",
        "val_filenames, val_num_samples = data_handler.get_filenames('data/val')\n",
        "\n",
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(data_handler.get_waveform_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  output_ds = output_ds.map(audio_processor.get_mel_tensor, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  return output_ds\n",
        "\n",
        "train_ds = preprocess_dataset(train_filenames)\n",
        "val_ds = preprocess_dataset(val_filenames)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "## early stop\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=20,\n",
        "                                                restore_best_weights=True)\n",
        "## learning rate decay callback\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(decay)\n",
        "callback_list = [early_stop, lr_schedule]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 251, 1)\n",
            "Number of total examples: 80508\n",
            "Example file tensor: tf.Tensor(b'data\\\\train\\\\4\\\\03-01-05-02-01-02-16_3840.wav', shape=(), dtype=string)\n",
            "Number of total examples: 9362\n",
            "Example file tensor: tf.Tensor(b'data\\\\val\\\\2\\\\03-01-03-02-01-02-08_0.wav', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw_NLOvhNv_6"
      },
      "source": [
        "model = MobileNet(input_shape=input_shape, classes=8)\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=val_ds,\n",
        "                    verbose=1,\n",
        "                    callbacks=callback_list\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwB5uScbEOYU"
      },
      "source": [
        "##visualize the preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLBPh3nSk5U6"
      },
      "source": [
        "train_gen = train_data_generator()\n",
        "testing_data = next(train_gen)[0]\n",
        "\n",
        "spec = audio_processor.spectrogram(testing_data)\n",
        "librosa.display.specshow(spec, sr=sample_freq, x_axis='time', y_axis='linear');\n",
        "plt.colorbar();\n",
        "plt.title('Spectrogram')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "mel_spec = audio_processor.mel_spectrogram(testing_data)\n",
        "librosa.display.specshow(mel_spec, sr=sample_freq, x_axis='time', y_axis='linear');\n",
        "plt.colorbar();\n",
        "plt.title('Mel Spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y8a9Uz59f-4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}